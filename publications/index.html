<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Research webpage of Adrien Taylor </title> <meta name="author" content="Adrien Taylor"> <meta name="description" content=""> <meta name="keywords" content="numerical optimization, numerical analysis, computational mathematics"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adrientaylor.github.io/publications/"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Research webpage of Adrien Taylor </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/tutorials/">tutorials </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description"></p> </header> <article> <p><strong>publications policy —</strong> I do my best to maintain updated versions with possible typo corrections and clarifications on arxiv (both are generally marked in bold and red for easy reference). Therefore, please favor the arxiv versions to the official published ones.</p> <p><strong>thesis —</strong> my thesis (under the supervision of <a href="http://perso.uclouvain.be/francois.glineur" rel="external nofollow noopener" target="_blank">François Glineur</a> and <a href="http://perso.uclouvain.be/julien.hendrickx" rel="external nofollow noopener" target="_blank">Julien Hendrickx</a>) had the chance to be awarded the <a href="https://uclouvain.be/en/research-institutes/icteam/icteam-thesis-awards-since-2011.html" rel="external nofollow noopener" target="_blank">ICTEAM thesis award</a> for 2018, the IBM-FNRS innovation award for 2018, and to be a finalist for the <a href="http://www.mathopt.org/?nav=tucker" rel="external nofollow noopener" target="_blank">AW Tucker prize</a> for 2018. In addition, we received the <a href="https://link.springer.com/article/10.1007/s11590-018-1379-y" rel="external nofollow noopener" target="_blank">2017 best paper award</a> in Optimization Letters, for a joint work with <a href="https://sites.google.com/site/homepageetiennedeklerk/" rel="external nofollow noopener" target="_blank">Etienne de Klerk</a> and <a href="http://perso.uclouvain.be/francois.glineur" rel="external nofollow noopener" target="_blank">François Glineur</a> (for <a href="https://link.springer.com/article/10.1007/s11590-016-1087-4" rel="external nofollow noopener" target="_blank">this paper</a>).</p> <p><strong>codes —</strong> see my <a href="https://github.com/AdrienTaylor" rel="external nofollow noopener" target="_blank">github profile</a> for all my codes. The current version of the <em>Performance EStimation TOolbox</em> (PESTO) is available from <a href="https://github.com/AdrienTaylor/Performance-Estimation-Toolbox" rel="external nofollow noopener" target="_blank">here</a> (<a href="https://github.com/AdrienTaylor/Performance-Estimation-Toolbox/blob/master/UserGuide.pdf" rel="external nofollow noopener" target="_blank">user manual</a>, <a href="https://perso.uclouvain.be/julien.hendrickx/availablepublications/PESTO_CDC_2017.pdf" rel="external nofollow noopener" target="_blank">conference proceeding</a>). The numerical worst-case analyses from PEP can now be performed just by writting the algorithms just as you would implement them in Matlab. The new <em>PEPit</em> (performance estimation in Python) is available from <a href="https://github.com/bgoujaud/PEPit" rel="external nofollow noopener" target="_blank">here</a> (due to the fabulous work of Baptiste Goujaud and Céline Moucer). It is easy to experiment with it using this <a href="https://github.com/bgoujaud/PEPit/blob/master/ressources/demo/PEPit_demo.ipynb" rel="external nofollow noopener" target="_blank">notebook</a> (see <a href="https://colab.research.google.com/github/bgoujaud/PEPit/blob/master/ressources/demo/PEPit_demo.ipynb" rel="external nofollow noopener" target="_blank">colab</a>).</p> <div class="publications"> <h2 class="bibliography">1 - preprints</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">preprint</abbr> </div> <div id="gupta2023nonlinear" class="col-sm-8"> <div class="title">Nonlinear conjugate gradient methods: worst-case convergence rates via computer-assisted analyses</div> <div class="author"> <a href="https://shuvomoy.github.io/" rel="external nofollow noopener" target="_blank">Shuvomoy Das Gupta</a> ,  <a href="https://mitmgmtfaculty.mit.edu/rfreund/" rel="external nofollow noopener" target="_blank">Robert M. Freund</a> ,  <a href="https://sites.google.com/view/asunlab/" rel="external nofollow noopener" target="_blank">X. Andy Sun</a> ,  and  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> </div> <div class="periodical"> <em>arXiv:2301.01530</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2301.01530" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/Shuvomoy/NCG-PEP-code" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We propose a computer-assisted approach to the analysis of the worst-case convergence of nonlinear conjugate gradient methods (NCGMs). Those methods are known for their generally good empirical performances for large-scale optimization, while having relatively incomplete analyses. Using our computer-assisted approach, we establish novel complexity bounds for the Polak-Ribi‘ere-Polyak (PRP) and the Fletcher-Reeves (FR) NCGMs for smooth strongly convex minimization. Conversely, we provide examples showing that those methods might behave worse than the regular steepest descent on the same class of problems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">gupta2023nonlinear</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Nonlinear conjugate gradient methods: worst-case convergence rates via computer-assisted analyses}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Das Gupta, Shuvomoy and Freund, Robert M. and Sun, X. Andy and Taylor, Adrien B.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv:2301.01530}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">preprint</abbr> </div> <div id="bambade2023proxqp" class="col-sm-8"> <div class="title">PROXQP: an Efficient and Versatile Quadratic Programming Solver for Real-Time Robotics Applications and Beyond</div> <div class="author"> <a href="https://bambade.github.io" rel="external nofollow noopener" target="_blank">Antoine Bambade</a> ,  <a href="https://fabinsch.github.io/" rel="external nofollow noopener" target="_blank">Fabian Schramm</a> ,  <a href="https://www.linkedin.com/in/sarah-kazdadi-059b94210/?originalSubdomain=fr" rel="external nofollow noopener" target="_blank">Sarah El Kazdadi</a> ,  <a href="https://scaron.info/" rel="external nofollow noopener" target="_blank">Stéphane Caron</a> ,  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  and  <a href="https://jcarpent.github.io/" rel="external nofollow noopener" target="_blank">Justin Carpentier</a> </div> <div class="periodical"> <em></em> 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://inria.hal.science/hal-04198663/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/Simple-Robotics/proxsuite" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Convex Quadratic programming (QP) has become a core component in the modern engineering toolkit, particularly in robotics, where QP problems are legions, ranging from real-time whole-body controllers to planning and estimation algorithms. Many of those QPs need to be solved at high frequency. Meeting timing requirements requires taking advantage of as many structural properties as possible for the problem at hand. For instance, it is generally crucial to resort to warm-starting to exploit the resemblance of consecutive control iterations. While a large range of off-the-shelf QP solvers is available, only a few are suited to exploit problem structure and warm-starting capacities adequately. In this work, we propose the PROXQP algorithm, a new and efficient QP solver that exploits QP structures by leveraging primal-dual augmented Lagrangian techniques. For convex QPs, PROXQP features a global convergence guarantee to the closest feasible QP, an essential property for safe closedloop control. We illustrate its practical performance on various standard robotic and control experiments, including a real-world closed-loop model predictive control application. While originally tailored for robotics applications, we show that PROXQP also performs at the level of state of the art on generic QP problems, making PROXQP suitable for use as an off-the-shelf solver for regular applications beyond robotics.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">bambade2023proxqp</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PROXQP: an Efficient and Versatile Quadratic Programming Solver for Real-Time Robotics Applications and Beyond}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bambade, Antoine and Schramm, Fabian and El Kazdadi, Sarah and Caron, St{\'e}phane and Taylor, Adrien B. and Carpentier, Justin}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">preprint</abbr> </div> <div id="goujaud2023provable" class="col-sm-8"> <div class="title">Provable non-accelerations of the heavy-ball method</div> <div class="author"> <a href="https://scholar.google.com/citations?user=93PAG2AAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Baptiste Goujaud</a> ,  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  and  <a href="http://www.cmap.polytechnique.fr/~aymeric.dieuleveut/" rel="external nofollow noopener" target="_blank">Aymeric Dieuleveut</a> </div> <div class="periodical"> <em>arXiv:2307.11291</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2307.11291" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>In this work, we show that the heavy-ball (\HB) method provably does not reach an accelerated convergence rate on smooth strongly convex problems. More specifically, we show that for any condition number and any choice of algorithmic parameters, either the worst-case convergence rate of \HB on the class of -smooth and -strongly convex quadratic functions is not accelerated (that is, slower than), or there exists an -smooth -strongly convex function and an initialization such that the method does not converge. To the best of our knowledge, this result closes a simple yet open question on one of the most used and iconic first-order optimization technique. Our approach builds on finding functions for which \HB fails to converge and instead cycles over finitely many iterates. We analytically describe all parametrizations of \HB that exhibit this cycling behavior on a particular cycle shape, whose choice is supported by a systematic and constructive approach to the study of cycling behaviors of first-order methods. We show the robustness of our results to perturbations of the cycle, and extend them to class of functions that also satisfy higher-order regularity conditions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">goujaud2023provable</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Provable non-accelerations of the heavy-ball method}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Goujaud, Baptiste and Taylor, Adrien B. and Dieuleveut, Aymeric}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv:2307.11291}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">preprint</abbr> </div> <div id="goujaud2022pepit" class="col-sm-8"> <div class="title">PEPit: computer-assisted worst-case analyses of first-order optimization methods in Python</div> <div class="author"> <a href="https://scholar.google.com/citations?user=93PAG2AAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Baptiste Goujaud</a> ,  <a href="https://www.linkedin.com/in/c%C3%A9line-moucer-88068b173/?originalSubdomain=fr" rel="external nofollow noopener" target="_blank">Céline Moucer</a> ,  <a href="https://perso.uclouvain.be/francois.glineur/" rel="external nofollow noopener" target="_blank">François Glineur</a> ,  <a href="https://perso.uclouvain.be/julien.hendrickx" rel="external nofollow noopener" target="_blank">Julien M. Hendrickx</a> ,  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  and  <a href="http://www.cmap.polytechnique.fr/~aymeric.dieuleveut/" rel="external nofollow noopener" target="_blank">Aymeric Dieuleveut</a> </div> <div class="periodical"> <em>arXiv:2201.04040</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2201.04040" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/PerformanceEstimation/PEPit" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>PEPit is a python package aiming at simplifying the access to worst-case analyses of a large family of first-order optimization methods possibly involving gradient, projection, proximal, or linear optimization oracles, along with their approximate, or Bregman variants. In short, PEPit is a package enabling computer-assisted worst-case analyses of first- order optimization methods. The key underlying idea is to cast the problem of performing a worst-case analysis, often referred to as a performance estimation problem (PEP), as a semidefinite program (SDP) which can be solved numerically. For doing that, the package users are only required to write first-order methods nearly as they would have implemented them. The package then takes care of the SDP modelling parts, and the worst-case analysis is performed numerically via a standard solver.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">goujaud2022pepit</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PEPit: computer-assisted worst-case analyses of first-order optimization methods in Python}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Goujaud, Baptiste and Moucer, C{\'e}line and Glineur, Fran{\c{c}}ois and Hendrickx, Julien M. and Taylor, Adrien B. and Dieuleveut, Aymeric}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv:2201.04040}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">preprint</abbr> </div> <div id="goujad2022opt" class="col-sm-8"> <div class="title">Optimal first-order methods for convex functions with a quadratic upper bound</div> <div class="author"> <a href="https://scholar.google.com/citations?user=93PAG2AAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Baptiste Goujaud</a> ,  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  and  <a href="http://www.cmap.polytechnique.fr/~aymeric.dieuleveut/" rel="external nofollow noopener" target="_blank">Aymeric Dieuleveut</a> </div> <div class="periodical"> <em>arXiv:2205.15033</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2205.15033" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2205.15033.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/bgoujaud/QG" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We analyze worst-case convergence guarantees of first-order optimization methods over a function class extending that of smooth and convex functions. This class contains convex functions that admit a simple quadratic upper bound. Its study is motivated by its stability under minor perturbations. We provide a thorough analysis of first-order methods, including worst-case convergence guarantees for several algorithms, and demonstrate that some of them achieve the optimal worst-case guarantee over the class. We support our analysis by numerical validation of worst-case guarantees using performance estimation problems. A few observations can be drawn from this analysis, particularly regarding the optimality (resp. and adaptivity) of the heavy-ball method (resp. heavy-ball with line-search). Finally, we show how our analysis can be leveraged to obtain convergence guarantees over more complex classes of functions. Overall, this study brings insights on the choice of function classes over which standard first-order methods have working worst-case guarantees.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">goujad2022opt</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Optimal first-order methods for convex functions with a quadratic upper bound}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Goujaud, Baptiste and Taylor, Adrien B. and Dieuleveut, Aymeric}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv:2205.15033}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">preprint</abbr> </div> <div id="goujaud2022quadratic" class="col-sm-8"> <div class="title">Quadratic minimization: from conjugate gradient to an adaptive Heavy-ball method with Polyak step-sizes</div> <div class="author"> <a href="https://scholar.google.com/citations?user=93PAG2AAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Baptiste Goujaud</a> ,  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  and  <a href="http://www.cmap.polytechnique.fr/~aymeric.dieuleveut/" rel="external nofollow noopener" target="_blank">Aymeric Dieuleveut</a> </div> <div class="periodical"> <em>arXiv:2210.06367</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2210.06367" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.06367.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/bgoujaud/Heavy-ball_polyak_steps" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>In this work, we propose an adaptive variation on the classical Heavy-ball method for convex quadratic minimization. The adaptivity crucially relies on so-called “Polyak step-sizes”, which consists in using the knowledge of the optimal value of the optimization problem at hand instead of problem parameters such as a few eigenvalues of the Hessian of the problem. This method happens to also be equivalent to a variation of the classical conjugate gradient method, and thereby inherits many of its attractive features, including its finite-time convergence, instance optimality, and its worst-case convergence rates. The classical gradient method with Polyak step-sizes is known to behave very well in situations in which it can be used, and the question of whether incorporating momentum in this method is possible and can improve the method itself appeared to be open. We provide a definitive answer to this question for minimizing convex quadratic functions, a arguably necessary first step for developing such methods in more general setups.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">goujaud2022quadratic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Quadratic minimization: from conjugate gradient to an adaptive Heavy-ball method with Polyak step-sizes}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Goujaud, Baptiste and Taylor, Adrien B. and Dieuleveut, Aymeric}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv:2210.06367}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2 - books</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">book</abbr> </div> <div id="daspremont2021acceleration" class="col-sm-8"> <div class="title">Acceleration Methods</div> <div class="author"> Alexandre d’Aspremont ,  <a href="https://damienscieur.com/" rel="external nofollow noopener" target="_blank">Damien Scieur</a> ,  and  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> </div> <div class="periodical"> <em>Foundations and Trends in Optimization</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2101.09545" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.nowpublishers.com/article/Details/OPT-036" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/AdrienTaylor/AccelerationMonograph" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>This monograph covers some recent advances in a range of acceleration techniques frequently used in convex optimization. We first use quadratic optimization problems to introduce two key families of methods, namely momentum and nested optimization schemes. They coincide in the quadratic case to form the Chebyshev method. We discuss momentum methods in detail, starting with the seminal work of Nesterov [1] and structure convergence proofs using a few master templates, such as that for optimized gradient methods, which provide the key benefit of showing how momentum methods optimize convergence guarantees. We further cover proximal acceleration, at the heart of the Catalyst and Accelerated Hybrid Proximal Extragradient frameworks, using similar algorithmic patterns. Common acceleration techniques rely directly on the knowledge of some of the regularity parameters in the problem at hand. We conclude by discussing restart schemes, a set of simple techniques for reaching nearly optimal convergence rates while adapting to unobserved regularity parameters.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">daspremont2021acceleration</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Foundations and Trends in Optimization}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Acceleration Methods}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1-2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-245}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{d’Aspremont, Alexandre and Scieur, Damien and Taylor, Adrien B.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">3 - journals</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">journal</abbr> </div> <div id="upadhyaya2023automated" class="col-sm-8"> <div class="title">Automated tight Lyapunov analysis for first-order methods</div> <div class="author"> <a href="https://manuupadhyaya.github.io/" rel="external nofollow noopener" target="_blank">Manu Upadhyaya</a> ,  <a href="https://github.com/sbanert" rel="external nofollow noopener" target="_blank">Sebastian Banert</a> ,  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  and  <a href="https://www.control.lth.se/personnel-old/pontus-giselsson/" rel="external nofollow noopener" target="_blank">Pontus Giselsson</a> </div> <div class="periodical"> <em>Mathematical Programming (to appear)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2302.06713" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/ManuUpadhyaya/tight_lyapunov_analysis" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We present a methodology for establishing the existence of quadratic Lyapunov inequalities for a wide range of first-order methods used to solve convex optimization problems. In particular, we consider i) classes of optimization problems of finite-sum form with (possibly strongly) convex and possibly smooth functional components, ii) first-order methods that can be written as a linear system on state-space form in feedback interconnection with the subdifferentials of the functional components of the objective function, and iii) quadratic Lyapunov inequalities that can be used to draw convergence conclusions. We provide a necessary and sufficient condition for the existence of a quadratic Lyapunov inequality that amounts to solving a small-sized semidefinite program. We showcase our methodology on several first-order methods that fit the framework. Most notably, our methodology allows us to significantly extend the region of parameter choices that allow for duality gap convergence in the Chambolle-Pock method when the linear operator is the identity mapping.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">upadhyaya2023automated</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automated tight Lyapunov analysis for first-order methods}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Upadhyaya, Manu and Banert, Sebastian and Taylor, Adrien B. and Giselsson, Pontus}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Mathematical Programming (to appear)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">journal</abbr> </div> <div id="goujaud2023counter" class="col-sm-8"> <div class="title">Counter-examples in first-order optimization: a constructive approach</div> <div class="author"> <a href="https://scholar.google.com/citations?user=93PAG2AAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Baptiste Goujaud</a> ,  <a href="http://www.cmap.polytechnique.fr/~aymeric.dieuleveut/" rel="external nofollow noopener" target="_blank">Aymeric Dieuleveut</a> ,  and  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> </div> <div class="periodical"> <em>IEEE Control Systems Letters</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2303.10503" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/10153415" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2303.10503.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/bgoujaud/cycles" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>While many approaches were developed for obtaining worst-case complexity bounds for first-order optimization methods in the last years, there remain theoretical gaps in cases where no such bound can be found. In such cases, it is often unclear whether no such bound exists (e.g., because the algorithm might fail to systematically converge) or simply if the current techniques do not allow finding them. In this work, we propose an approach to automate the search for cyclic trajectories generated by first-order methods. This provides a constructive approach to show that no appropriate complexity bound exists, thereby complementing the approaches providing sufficient conditions for convergence. Using this tool, we provide ranges of parameters for which some of the famous heavy-ball, Nesterov accelerated gradient, inexact gradient descent, and three-operator splitting algorithms fail to systematically converge, and show that it nicely complements existing tools searching for Lyapunov functions. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">goujaud2023counter</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Counter-examples in first-order optimization: a constructive approach}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Goujaud, Baptiste and Dieuleveut, Aymeric and Taylor, Adrien B.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Control Systems Letters}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">journal</abbr> </div> <div id="moucer2022systematic" class="col-sm-8"> <div class="title">A systematic approach to Lyapunov analyses of continuous-time models in convex optimization</div> <div class="author"> <a href="https://www.linkedin.com/in/c%C3%A9line-moucer-88068b173/?originalSubdomain=fr" rel="external nofollow noopener" target="_blank">Céline Moucer</a> ,  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  and  <a href="https://www.di.ens.fr/~fbach/" rel="external nofollow noopener" target="_blank">Francis Bach</a> </div> <div class="periodical"> <em>SIAM Journal on Optimization</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2205.12772" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2205.12772.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/CMoucer/PEP_ODEs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>First-order methods are often analyzed via their continuous-time models, where their worst-case convergence properties are usually approached via Lyapunov functions. In this work, we provide a systematic and principled approach to find and verify Lyapunov functions for classes of ordinary and stochastic differential equations. More precisely, we extend the performance estimation framework, originally proposed by Drori and Teboulle [10], to continuous-time models. We retrieve convergence results comparable to those of discrete methods using fewer assumptions and convexity inequalities, and provide new results for stochastic accelerated gradient flows.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">moucer2022systematic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A systematic approach to Lyapunov analyses of continuous-time models in convex optimization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Moucer, Céline and Taylor, Adrien B. and Bach, Francis}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{SIAM Journal on Optimization}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">journal</abbr> </div> <div id="drori2023optimal" class="col-sm-8"> <div class="title">An optimal gradient method for smooth strongly convex minimization</div> <div class="author"> <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  and  <a href="https://scholar.google.com/citations?user=7pRQY3MAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Yoel Drori</a> </div> <div class="periodical"> <em>Mathematical Programming</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2101.09741" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/article/10.1007/s10107-022-01839-y" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/AdrienTaylor/Optimal-Gradient-Method" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We present an optimal gradient method for smooth strongly convex optimization. The method is optimal in the sense that its worst-case bound on the distance to an optimal point exactly matches the lower bound on the oracle complexity for the class of problems, meaning that no black-box first-order method can have a better worst-case guarantee without further assumptions on the class of problems at hand. In addition, we provide a constructive recipe for obtaining the algorithmic parameters of the method and illustrate that it can be used for deriving methods for other optimality criteria as well. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">drori2023optimal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{An optimal gradient method for smooth strongly convex minimization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Taylor, Adrien B. and Drori, Yoel}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Mathematical Programming}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{199}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1-2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{557--594}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">journal</abbr> </div> <div id="barre2023principled" class="col-sm-8"> <div class="title">Principled Analyses and Design of First-Order Methods with Inexact Proximal Operators</div> <div class="author"> <a href="https://mathbarre.github.io/" rel="external nofollow noopener" target="_blank">Mathieu Barré</a> ,  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  and  <a href="https://www.di.ens.fr/~fbach/" rel="external nofollow noopener" target="_blank">Francis Bach</a> </div> <div class="periodical"> <em>Mathematical Programming</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2006.06041" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/mathbarre/InexactProximalOperators" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Proximal operations are among the most common primitives appearing in both practical and theoretical (or high-level) optimization methods. This basic operation typically consists in solving an intermediary (hopefully simpler) optimization problem. In this work, we survey notions of inaccuracies that can be used when solving those intermediary optimization problems. Then, we show that worst-case guarantees for algorithms relying on such inexact proximal operations can be systematically obtained through a generic procedure based on semidefinite programming. This methodology is primarily based on the approach introduced by Drori and Teboulle (2014) and on convex interpolation results, and allows producing non-improvable worst-case analyzes. In other words, for a given algorithm, the methodology generates both worst-case certificates (i.e., proofs) and problem instances on which those bounds are achieved. Relying on this methodology, we study numerical worst-case performances of a few basic methods relying on inexact proximal operations including accelerated variants, and design a variant with optimized worst-case behaviour. We further illustrate how to extend the approach to support strongly convex objectives by studying a simple relatively inexact proximal minimization method. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">barre2023principled</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Principled Analyses and Design of First-Order Methods with Inexact Proximal Operators}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Barré, Mathieu and Taylor, Adrien B. and Bach, Francis}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Mathematical Programming}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{201}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1-2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{185--230}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">journal</abbr> </div> <div id="drori2022oracle" class="col-sm-8"> <div class="title">On the oracle complexity of smooth strongly convex minimization</div> <div class="author"> <a href="https://scholar.google.com/citations?user=7pRQY3MAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Yoel Drori</a> ,  and  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> </div> <div class="periodical"> <em>Journal of Complexity</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2101.09740" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S0885064X21000455" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>We construct a family of functions suitable for establishing lower bounds on the oracle complexity of first-order minimization of smooth strongly-convex functions. Based on this construction, we derive new lower bounds on the complexity of strongly-convex minimization under various inaccuracy criteria. The new bounds match the known upper bounds up to a constant factor, and when the inaccuracy of a solution is measured by its distance to the solution set, the new lower bound exactly matches the upper bound obtained by the recent Information-Theoretic Exact Method by the same authors, thereby establishing the exact oracle complexity for this class of problems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">drori2022oracle</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On the oracle complexity of smooth strongly convex minimization}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Complexity}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{68}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{101590}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Drori, Yoel and Taylor, Adrien B.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">journal</abbr> </div> <div id="barre2020note" class="col-sm-8"> <div class="title">A note on approximate accelerated forward-backward methods with absolute and relative errors, and possibly strongly convex objectives</div> <div class="author"> <a href="https://mathbarre.github.io/" rel="external nofollow noopener" target="_blank">Mathieu Barré</a> ,  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  and  <a href="https://www.di.ens.fr/~fbach/" rel="external nofollow noopener" target="_blank">Francis Bach</a> </div> <div class="periodical"> <em>Open Journal of Mathematical Optimization</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2106.15536" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ojmo.centre-mersenne.org/item/10.5802/ojmo.12.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/mathbarre/StronglyConvexForwardBackward" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>In this short note, we provide a simple version of an accelerated forward-backward method (a.k.a. Nesterov’s accelerated proximal gradient method) possibly relying on approximate proximal operators and allowing to exploit strong convexity of the objective function. The method supports both relative and absolute errors, and its behavior is illustrated on a set of standard numerical experiments. Using the same developments, we further provide a version of the accelerated proximal hybrid extragradient method of Monteiro and Svaiter (2013) possibly exploiting strong convexity of the objective function.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">barre2020note</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Barré, Mathieu and Taylor, Adrien B. and Bach, Francis}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A note on approximate accelerated forward-backward methods with absolute and relative errors, and possibly strongly convex objectives}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Open Journal of Mathematical Optimization}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">journal</abbr> </div> <div id="Barr20" class="col-sm-8"> <div class="title">Convergence of a Constrained Vector Extrapolation Scheme</div> <div class="author"> <a href="https://mathbarre.github.io/" rel="external nofollow noopener" target="_blank">Mathieu Barré</a> ,  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  and  Alexandre d’Aspremont </div> <div class="periodical"> <em>SIAM Journal on Mathematics of Data Science</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2010.15482" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://epubs.siam.org/doi/abs/10.1137/21M1428030" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>We prove non asymptotic linear convergence rates for the constrained Anderson acceleration extrapolation scheme. These guarantees come from new upper bounds on the constrained Chebyshev problem, which consists in minimizing the maximum absolute value of a polynomial on a bounded real interval with l1 constraints on its coefficients vector. Constrained Anderson Acceleration has a numerical cost comparable to that of the original scheme.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Barr20</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Barré, Mathieu and Taylor, Adrien B. and d’Aspremont, Alexandre}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{SIAM Journal on Mathematics of Data Science}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{979-1002}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{SIAM}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Convergence of a {C}onstrained {V}ector {E}xtrapolation {S}cheme}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">journal</abbr> </div> <div id="dragomir2021optimal" class="col-sm-8"> <div class="title">Optimal complexity and certification of Bregman first-order methods</div> <div class="author"> <a href="https://perso.uclouvain.be/radu.dragomir/" rel="external nofollow noopener" target="_blank">Radu-Alexandru Dragomir</a> ,  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  Alexandre d’Aspremont ,  and  <a href="https://www.tse-fr.eu/fr/people/jerome-bolte" rel="external nofollow noopener" target="_blank">Jérôme Bolte</a> </div> <div class="periodical"> <em>Mathematical Programming</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1911.08510" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/article/10.1007/s10107-021-01618-1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/RaduAlexandruDragomir/BregmanPerformanceEstimation" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We provide a lower bound showing that the O(1/k) convergence rate of the NoLips method (a.k.a. Bregman Gradient) is optimal for the class of functions satisfying the h-smoothness assumption. This assumption, also known as relative smoothness, appeared in the recent developments around the Bregman Gradient method, where acceleration remained an open issue. On the way, we show how to constructively obtain the corresponding worst-case functions by extending the computer-assisted performance estimation framework of Drori and Teboulle (Mathematical Programming, 2014) to Bregman first-order methods, and to handle the classes of differentiable and strictly convex functions. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">dragomir2021optimal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Optimal complexity and certification of Bregman first-order methods}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dragomir, Radu-Alexandru and Taylor, Adrien B. and d’Aspremont, Alexandre and Bolte, J{\'e}r{\^o}me}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Mathematical Programming}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--43}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">journal</abbr> </div> <div id="drori2019efficient" class="col-sm-8"> <div class="title">Efficient first-order methods for convex minimization: a constructive approach</div> <div class="author"> <a href="https://scholar.google.com/citations?user=7pRQY3MAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Yoel Drori</a> ,  and  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> </div> <div class="periodical"> <em>Mathematical Programming</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1803.05676" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/article/10.1007/s10107-019-01410-2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/AdrienTaylor/GreedyMethods" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We describe a novel constructive technique for devising efficient first-order methods for a wide range of large-scale convex minimization settings, including smooth, non-smooth, and strongly convex minimization. The technique builds upon a certain variant of the conjugate gradient method to construct a family of methods such that a) all methods in the family share the same worst-case guarantee as the base conjugate gradient method, and b) the family includes a fixed-step first-order method. We demonstrate the effectiveness of the approach by deriving optimal methods for the smooth and non-smooth cases, including new methods that forego knowledge of the problem parameters at the cost of a one-dimensional line search per iteration, and a universal method for the union of these classes that requires a three-dimensional search per iteration. In the strongly convex case, we show how numerical tools can be used to perform the construction, and show that the resulting method offers an improved worst-case bound compared to Nesterov’s celebrated fast gradient method.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">drori2019efficient</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Efficient first-order methods for convex minimization: a constructive approach}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Drori, Yoel and Taylor, Adrien B.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Mathematical Programming}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{184}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{183--220}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">journal</abbr> </div> <div id="ryu2020operator" class="col-sm-8"> <div class="title">Operator splitting performance estimation: Tight contraction factors and optimal parameter selection</div> <div class="author"> <a href="https://ernestryu.com/" rel="external nofollow noopener" target="_blank">Ernest K. Ryu</a> ,  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  <a href="http://www.control.lth.se/Staff/carolina-bergeling.html" rel="external nofollow noopener" target="_blank">Carolina Bergeling</a> ,  and  <a href="https://www.control.lth.se/personnel-old/pontus-giselsson/" rel="external nofollow noopener" target="_blank">Pontus Giselsson</a> </div> <div class="periodical"> <em>SIAM Journal on Optimization</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1812.00146" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://epubs.siam.org/doi/abs/10.1137/19M1304854" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/AdrienTaylor/OperatorSplittingPerformanceEstimation" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We propose a methodology for studying the performance of common splitting methods through semidefinite programming. We prove tightness of the methodology and demonstrate its value by presenting two applications of it. First, we use the methodology as a tool for computer-assisted proofs to prove tight analytical contraction factors for Douglas–Rachford splitting that are likely too complicated for a human to find bare-handed. Second, we use the methodology as an algorithmic tool to computationally select the optimal splitting method parameters by solving a series of semidefinite programs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ryu2020operator</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Operator splitting performance estimation: Tight contraction factors and optimal parameter selection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ryu, Ernest K. and Taylor, Adrien B. and Bergeling, Carolina and Giselsson, Pontus}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{SIAM Journal on Optimization}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{30}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2251--2271}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{SIAM}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">journal</abbr> </div> <div id="de2020worst" class="col-sm-8"> <div class="title">Worst-case convergence analysis of inexact gradient and Newton methods through semidefinite programming performance estimation</div> <div class="author"> <a href="https://sites.google.com/site/homepageetiennedeklerk/" rel="external nofollow noopener" target="_blank">Etienne De Klerk</a> ,  <a href="https://perso.uclouvain.be/francois.glineur/" rel="external nofollow noopener" target="_blank">François Glineur</a> ,  and  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> </div> <div class="periodical"> <em>SIAM Journal on Optimization</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1709.05191" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://epubs.siam.org/doi/abs/10.1137/19M1281368" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/AdrienTaylor/Performance-Estimation-Problems-For-Newton" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We provide new tools for worst-case performance analysis of the gradient (or steepest descent) method of Cauchy for smooth strongly convex functions, and Newton’s method for self-concordant functions, including the case of inexact search directions. The analysis uses semidefinite programming performance estimation, as pioneered by Drori and Teboulle [Mathematical Programming, 145(1-2):451–482, 2014], and extends recent performance estimation results for the method of Cauchy by the authors [Optimization Letters, 11(7), 1185-1199, 2017]. To illustrate the applicability of the tools, we demonstrate a novel complexity analysis of short step interior point methods using inexact search directions. As an example in this framework, we sketch how to give a rigorous worst-case complexity analysis of a recent interior point method by Abernethy and Hazan [PMLR, 48:2520–2528, 2016].</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">de2020worst</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Worst-case convergence analysis of inexact gradient and Newton methods through semidefinite programming performance estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{De Klerk, Etienne and Glineur, Fran\c{c}ois and Taylor, Adrien B.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{SIAM Journal on Optimization}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{30}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2053--2082}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{SIAM}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">journal</abbr> </div> <div id="taylor2018exact" class="col-sm-8"> <div class="title">Exact worst-case convergence rates of the proximal gradient method for composite convex minimization</div> <div class="author"> <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  <a href="https://perso.uclouvain.be/julien.hendrickx" rel="external nofollow noopener" target="_blank">Julien M. Hendrickx</a> ,  and  <a href="https://perso.uclouvain.be/francois.glineur/" rel="external nofollow noopener" target="_blank">François Glineur</a> </div> <div class="periodical"> <em>Journal of Optimization Theory and Applications</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1705.04398" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://link.springer.com/content/pdf/10.1007%2Fs10957-018-1298-1.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/AdrienTaylor/ProximalGradientMethod" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p> We study the worst-case convergence rates of the proximal gradient method for minimizing the sum of a smooth strongly convex function and a non-smooth convex function whose proximal operator is available. We establish the exact worst-case convergence rates of the proximal gradient method in this setting for any step size and for different standard performance measures: objective function accuracy, distance to optimality and residual gradient norm. The proof methodology relies on recent developments in performance estimation of first-order methods based on semidefinite programming. In the case of the proximal gradient method, this methodology allows obtaining exact and non-asymptotic worst-case guarantees that are conceptually very simple, although apparently new. On the way, we discuss how strong convexity can be replaced by weaker assumptions, while preserving the corresponding convergence rates. We also establish that the same fixed step size policy is optimal for all three performance measures. Finally, we extend recent results on the worst-case behavior of gradient descent with exact line search to the proximal case.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">journal</abbr> </div> <div id="taylor2017exact" class="col-sm-8"> <div class="title">Exact worst-case performance of first-order methods for composite convex optimization</div> <div class="author"> <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  <a href="https://perso.uclouvain.be/julien.hendrickx" rel="external nofollow noopener" target="_blank">Julien M. Hendrickx</a> ,  and  <a href="https://perso.uclouvain.be/francois.glineur/" rel="external nofollow noopener" target="_blank">François Glineur</a> </div> <div class="periodical"> <em>SIAM Journal on Optimization</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1512.07516" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="http://epubs.siam.org/doi/10.1137/16M108104X" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/AdrienTaylor/Composite-Performance-Estimation-Problems-first-order-methods" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We provide a framework for computing the exact worst-case performance of any algorithm belonging to a broad class of oracle-based first-order methods for composite convex optimization, including those performing explicit, projected, proximal, conditional and inexact (sub)gradient steps. We simultaneously obtain tight worst-case guarantees and explicit instances of optimization problems on which the algorithm reaches this worst-case. We achieve this by reducing the computation of the worst-case to solving a convex semidefinite program, generalizing previous works on performance estimation by Drori and Teboulle [13] and the authors [43]. We use these developments to obtain a tighter analysis of the proximal point algorithm and of several variants of fast proximal gradient, conditional gradient, subgradient and alternating projection methods. In particular, we present a new analytical worst-case guarantee for the proximal point algorithm that is twice better than previously known, and improve the standard worst-case guarantee for the conditional gradient method by more than a factor of two. We also show how the optimized gradient method proposed by Kim and Fessler in [22] can be extended by incorporating a projection or a proximal operator, which leads to an algorithm that converges in the worst-case twice as fast as the standard accelerated proximal gradient method [2].</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">journal</abbr> </div> <div id="de2017worst" class="col-sm-8"> <div class="title">On the worst-case complexity of the gradient method with exact line search for smooth strongly convex functions [<b><a href="https://link.springer.com/article/10.1007/s11590-018-1379-y" rel="external nofollow noopener" target="_blank">Best paper award</a></b>]</div> <div class="author"> <a href="https://sites.google.com/site/homepageetiennedeklerk/" rel="external nofollow noopener" target="_blank">Etienne De Klerk</a> ,  <a href="https://perso.uclouvain.be/francois.glineur/" rel="external nofollow noopener" target="_blank">François Glineur</a> ,  and  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> </div> <div class="periodical"> <em>Optimization Letters</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://link.springer.com/article/10.1007/s11590-016-1087-4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://link.springer.com/content/pdf/10.1007/s11590-016-1087-4.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We consider the gradient (or steepest) descent method with exact line search applied to a strongly convex function with Lipschitz continuous gradient. We establish the exact worst-case rate of convergence of this scheme, and show that this worst-case behavior is exhibited by a certain convex quadratic function. We also give the tight worst-case complexity bound for a noisy variant of gradient descent method, where exact line-search is performed in a search direction that differs from negative gradient by at most a prescribed relative tolerance. The proofs are computer-assisted, and rely on the resolutions of semidefinite programming performance estimation problems as introduced in the paper (Drori and Teboulle, Math Progr 145(1–2):451–482, 2014).</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">journal</abbr> </div> <div id="taylor2017smooth" class="col-sm-8"> <div class="title">Smooth strongly convex interpolation and exact worst-case performance of first-order methods</div> <div class="author"> <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  <a href="https://perso.uclouvain.be/julien.hendrickx" rel="external nofollow noopener" target="_blank">Julien M. Hendrickx</a> ,  and  <a href="https://perso.uclouvain.be/francois.glineur/" rel="external nofollow noopener" target="_blank">François Glineur</a> </div> <div class="periodical"> <em>Mathematical Programming</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1502.05666" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="http://link.springer.com/article/10.1007/s10107-016-1009-3" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/AdrienTaylor/Unconstrained-Performance-Estimation-Problems-first-order-methods-" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We show that the exact worst-case performance of fixed-step first-order methods for unconstrained optimization of smooth (possibly strongly) convex functions can be obtained by solving convex programs. Finding the worst-case performance of a black-box first-order method is formulated as an optimization problem over a set of smooth (strongly) convex functions and initial conditions. We develop closed-form necessary and sufficient conditions for smooth (strongly) convex interpolation, which provide a finite representation for those functions. This allows us to reformulate the worst-case performance estimation problem as an equivalent finite dimension-independent semidefinite optimization problem, whose exact solution can be recovered up to numerical precision. Optimal solutions to this performance estimation problem provide both worst-case performance bounds and explicit functions matching them, as our smooth (strongly) convex interpolation procedure is constructive. Our works build on those of Drori and Teboulle in [Math. Prog. 145 (1-2), 2014] who introduced and solved relaxations of the performance estimation problem for smooth convex functions. We apply our approach to different fixed-step first-order methods with several performance criteria, including objective function accuracy and gradient norm. We conjecture several numerically supported worst-case bounds on the performance of the fixed-step gradient, fast gradient and optimized gradient methods, both in the smooth convex and the smooth strongly convex cases, and deduce tight estimates of the optimal step size for the gradient method.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">4 - conference</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">conference</abbr> </div> <div id="bambade2023qplayer" class="col-sm-8"> <div class="title">QPLayer: efficient differentiation of convex quadratic optimization</div> <div class="author"> <a href="https://bambade.github.io" rel="external nofollow noopener" target="_blank">Antoine Bambade</a> ,  <a href="https://fabinsch.github.io/" rel="external nofollow noopener" target="_blank">Fabian Schramm</a> ,  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  and  <a href="https://jcarpent.github.io/" rel="external nofollow noopener" target="_blank">Justin Carpentier</a> </div> <div class="periodical"> <em>In International Conference on Learning Representations (ICLR, to appear)</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/%20" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://inria.hal.science/hal-04133055/document" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Simple-Robotics/proxsuite" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Optimization layers within neural network architectures have become increasingly popular for their ability to solve a wide range of machine learning tasks and to model domain-specific knowledge. However, designing optimization layers requires careful consideration as the underlying optimization problems might be infeasible during training. Motivated by applications in learning, control, and robotics, this work focuses on convex quadratic programming (QP) layers. The specific structure of this type of optimization layer can be efficiently exploited for faster computations while still allowing rich modeling capabilities. We leverage primal-dual augmented Lagrangian techniques for computing derivatives of both feasible and infeasible QPs. Not requiring feasibility allows, as a byproduct, for more flexibility in the QP to be learned. The effectiveness of our approach is demonstrated in a few standard learning experiments, obtaining three to ten times faster computations than alternative state-of-the-art methods while being more accurate and numerically robust. Along with these contributions, we provide an open-source C++ software package called QPLayer for efficiently differentiating convex QPs and which can be interfaced with modern learning frameworks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bambade2023qplayer</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{QPLayer: efficient differentiation of convex quadratic optimization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bambade, Antoine and Schramm, Fabian and Taylor, Adrien B. and Carpentier, Justin}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations (ICLR, to appear)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">conference</abbr> </div> <div id="goujaud2023fundamental" class="col-sm-8"> <div class="title">On Fundamental Proof Structures in First-Order Optimization</div> <div class="author"> <a href="https://scholar.google.com/citations?user=93PAG2AAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Baptiste Goujaud</a> ,  <a href="http://www.cmap.polytechnique.fr/~aymeric.dieuleveut/" rel="external nofollow noopener" target="_blank">Aymeric Dieuleveut</a> ,  and  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> </div> <div class="periodical"> <em>In Proceedings of the 62nd Conference on Decision and Control (CDC)</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2310.02015" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2310.02015.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>First-order optimization methods have attracted a lot of attention due to their practical suc- cess in many applications, including in machine learning. Obtaining convergence guarantees and worst-case performance certificates for first-order methods have become crucial for understand- ing ingredients underlying efficient methods and for developing new ones. However, obtaining, verifying, and proving such guarantees is often a tedious task. Therefore, a few approaches were proposed for rendering this task more systematic, and even partially automated. In addition to helping researchers finding convergence proofs, these tools provide insights on the general struc- tures of such proofs. We aim at presenting those structures, showing how to build convergence guarantees for first-order optimization methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">goujaud2023fundamental</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On Fundamental Proof Structures in First-Order Optimization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Goujaud, Baptiste and Dieuleveut, Aymeric and Taylor, Adrien B.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 62nd Conference on Decision and Control (CDC)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3023--3030}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">conference</abbr> </div> <div id="gorbunov2023convergence" class="col-sm-8"> <div class="title">Convergence of Proximal Point and Extragradient-Based Methods Beyond Monotonicity: the Case of Negative Comonotonicity</div> <div class="author"> <a href="https://eduardgorbunov.github.io/" rel="external nofollow noopener" target="_blank">Eduard Gorbunov</a> ,  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  <a href="https://sites.google.com/view/samuelhorvath" rel="external nofollow noopener" target="_blank">Samuel Horvath</a> ,  and  <a href="https://gauthiergidel.github.io/" rel="external nofollow noopener" target="_blank">Gauthier Gidel</a> </div> <div class="periodical"> <em>In Proceedings of the 40th International Conference on Machine Learning (ICML)</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2210.13831" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2210.13831.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/eduardgorbunov/Proximal_Point_and_Extragradient_based_methods_negative_comonotonicity" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Algorithms for min-max optimization and variational inequalities are often studied under monotonicity assumptions. Motivated by non-monotone machine learning applications, we follow the line of works [Diakonikolas et al., 2021, Lee and Kim, 2021, Pethick et al., 2022, Böhm, 2022] aiming at going beyond monotonicity by considering the weaker negative comonotonicity assumption. In particular, we provide tight complexity analyses for the Proximal Point, Extragradient, and Optimistic Gradient methods in this setup, closing some questions on their working guarantees beyond monotonicity.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gorbunov2023convergence</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Convergence of Proximal Point and Extragradient-Based Methods Beyond Monotonicity: the Case of Negative Comonotonicity}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gorbunov, Eduard and Taylor, Adrien B. and Horvath, Samuel and Gidel, Gauthier}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 40th International Conference on Machine Learning (ICML)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">conference</abbr> </div> <div id="gorbunov2022last" class="col-sm-8"> <div class="title">Last-Iterate Convergence of Optimistic Gradient Method for Monotone Variational Inequalities</div> <div class="author"> <a href="https://eduardgorbunov.github.io/" rel="external nofollow noopener" target="_blank">Eduard Gorbunov</a> ,  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  and  <a href="https://gauthiergidel.github.io/" rel="external nofollow noopener" target="_blank">Gauthier Gidel</a> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems (NeurIPS)</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2205.08446" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2205.08446.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/eduardgorbunov/potentials_and_last_iter_convergence_for_VIPs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The Past Extragradient (PEG) [Popov, 1980] method, also known as the Optimistic Gradient method, has known a recent gain in interest in the optimization community with the emergence of variational inequality formulations for machine learning. Recently, in the unconstrained case, Golowich et al. [2020] proved that a O(1/N) last-iterate convergence rate in terms of the squared norm of the operator can be achieved for Lipschitz and monotone operators with a Lipschitz Jacobian. In this work, by introducing a novel analysis through potential functions, we show that (i) this O(1/N) last-iterate convergence can be achieved without any assumption on the Jacobian of the operator, and (ii) it can be extended to the constrained case, which was not derived before even under Lipschitzness of the Jacobian. The proof is significantly different from the one known from Golowich et al. [2020], and its discovery was computer-aided. Those results close the open question of the last iterate convergence of PEG for monotone variational inequalities.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gorbunov2022last</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Last-Iterate Convergence of Optimistic Gradient Method for Monotone Variational Inequalities}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gorbunov, Eduard and Taylor, Adrien B. and Gidel, Gauthier}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">conference</abbr> </div> <div id="dubois2022fast" class="col-sm-8"> <div class="title">Fast Stochastic Composite Minimization and an Accelerated Frank-Wolfe Algorithm under Parallelization</div> <div class="author"> <a href="https://bpauld.github.io/" rel="external nofollow noopener" target="_blank">Benjamin Dubois-Taine</a> ,  <a href="https://www.di.ens.fr/~fbach/" rel="external nofollow noopener" target="_blank">Francis Bach</a> ,  <a href="https://q-berthet.github.io/" rel="external nofollow noopener" target="_blank">Quentin Berthet</a> ,  and  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems (NeurIPS)</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2205.12751" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2205.12751" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/bpauld/PFW" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We consider the problem of minimizing the sum of two convex functions. One of those functions has Lipschitz-continuous gradients, and can be accessed via stochastic oracles, whereas the other is "simple". We provide a Bregman-type algorithm with accelerated convergence in function values to a ball containing the minimum. The radius of this ball depends on problem-dependent constants, including the variance of the stochastic oracle. We further show that this algorithmic setup naturally leads to a variant of Frank-Wolfe achieving acceleration under parallelization. More precisely, when minimizing a smooth convex function on a bounded domain, we show that one can achieve an εprimal-dual gap (in expectation) in \tildeO(1/\sqrt ε) iterations, by only accessing gradients of the original function and a linear maximization oracle with O(1/\sqrt ε) computing units in parallel. We illustrate this fast convergence on synthetic numerical experiments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">dubois2022fast</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fast Stochastic Composite Minimization and an Accelerated Frank-Wolfe Algorithm under Parallelization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dubois-Taine, Benjamin and Bach, Francis and Berthet, Quentin and Taylor, Adrien B.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">conference</abbr> </div> <div id="bambade2022prox" class="col-sm-8"> <div class="title">PROX-QP: Yet another Quadratic Programming Solver for Robotics and beyond</div> <div class="author"> <a href="https://bambade.github.io" rel="external nofollow noopener" target="_blank">Antoine Bambade</a> ,  <a href="https://www.linkedin.com/in/sarah-kazdadi-059b94210/?originalSubdomain=fr" rel="external nofollow noopener" target="_blank">Sarah El Kazdadi</a> ,  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  and  <a href="https://jcarpent.github.io/" rel="external nofollow noopener" target="_blank">Justin Carpentier</a> </div> <div class="periodical"> <em>In Robotics: Science and systems (RSS 2022)</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://www.roboticsproceedings.org/rss18/p040.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Bambade/proxqp_benchmark" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Quadratic programming (QP) has become a core modelling component in the modern engineering toolkit. This is particularly true for simulation, planning and control in robotics. Yet, modern numerical solvers have not reached the level of efficiency and reliability required in practical applications where speed, robustness, and accuracy are all necessary. In this work, we introduce a few variations of the well-established augmented Lagrangian method, specifically for solving QPs, which include heuristics for improving practical numerical performances. Those variants are embedded within an open-source software which includes an efficient C++ implementation, a modular API, as well as best-performing heuristics for our test-bed. Relying on this framework, we present a benchmark studying the practical performances of modern optimization solvers for convex QPs on generic and complex problems of the literature as well as on common robotic scenarios. This benchmark notably highlights that this approach outperforms modern solvers in terms of efficiency, accuracy and robustness for small to medium-sized problems, while remaining competitive for higher dimensions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bambade2022prox</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{PROX-QP: Yet another Quadratic Programming Solver for Robotics and beyond}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bambade, Antoine and El Kazdadi, Sarah and Taylor, Adrien B. and Carpentier, Justin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Robotics: Science and systems (RSS 2022)}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">conference</abbr> </div> <div id="goujaud2022super" class="col-sm-8"> <div class="title">Super-Acceleration with Cyclical Step-sizes</div> <div class="author"> <a href="https://scholar.google.com/citations?user=93PAG2AAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Baptiste Goujaud</a> ,  <a href="https://damienscieur.com/" rel="external nofollow noopener" target="_blank">Damien Scieur</a> ,  <a href="http://www.cmap.polytechnique.fr/~aymeric.dieuleveut/" rel="external nofollow noopener" target="_blank">Aymeric Dieuleveut</a> ,  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  and  <a href="https://fa.bianp.net/" rel="external nofollow noopener" target="_blank">Fabian Pedregosa</a> </div> <div class="periodical"> <em>In Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS)</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2106.09687" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/bgoujaud/Heavy_ball_cycling_step_sizes" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We develop a convergence-rate analysis of momentum with cyclical step-sizes. We show that under some assumption on the spectral gap of Hessians in machine learning, cyclical step-sizes are provably faster than constant step-sizes. More precisely, we develop a convergence rate analysis for quadratic objectives that provides optimal parameters and shows that cyclical learning rates can improve upon traditional lower complexity bounds. We further propose a systematic approach to design optimal first order methods for quadratic minimization with a given spectral structure. Finally, we provide a local convergence rate analysis beyond quadratic minimization for the proposed methods and illustrate our findings through benchmarks on least squares and logistic regression problems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">goujaud2022super</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Super-Acceleration with Cyclical Step-sizes}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Goujaud, Baptiste and Scieur, Damien and Dieuleveut, Aymeric and Taylor, Adrien B. and Pedregosa, Fabian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">conference</abbr> </div> <div id="even2021continuized" class="col-sm-8"> <div class="title">A Continuized View on Nesterov Acceleration for Stochastic Gradient Descent and Randomized Gossip [<b><a href="https://blog.neurips.cc/2021/11/30/announcing-the-neurips-2021-award-recipients/?s=09" rel="external nofollow noopener" target="_blank">Outstanding paper award</a></b>]</div> <div class="author"> <a href="https://mathieueven.netlify.app/" rel="external nofollow noopener" target="_blank">Mathieu Even</a> ,  <a href="https://raphael-berthier.github.io/" rel="external nofollow noopener" target="_blank">Raphaël Berthier</a> ,  <a href="https://www.di.ens.fr/~fbach/" rel="external nofollow noopener" target="_blank">Francis Bach</a> ,  <a href="https://www.di.ens.fr/~flammarion/" rel="external nofollow noopener" target="_blank">Nicolas Flammarion</a> ,  <a href="http://pierre.gaillard.me/research.html" rel="external nofollow noopener" target="_blank">Pierre Gaillard</a> ,  <a href="https://www.di.ens.fr/~hendrikx/" rel="external nofollow noopener" target="_blank">Hadrien Hendrikx</a> ,  <a href="https://www.di.ens.fr/laurent.massoulie/" rel="external nofollow noopener" target="_blank">Laurent Massoulié</a> ,  and  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems (NeurIPS)</em> , 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2106.07644" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://proceedings.neurips.cc/paper/2021/file/ec26fc2eb2b75aece19c70392dc744c2-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We introduce the "continuized"‘ Nesterov acceleration, a close variant of Nesterov acceleration whose variables are indexed by a continuous time parameter. The two variables continuously mix following a linear ordinary differential equation and take gradient steps at random times. This continuized variant benefits from the best of the continuous and the discrete frameworks: as a continuous process, one can use differential calculus to analyze convergence and obtain analytical expressions for the parameters; and a discretization of the continuized process can be computed exactly with convergence rates similar to those of Nesterov original acceleration. We show that the discretization has the same structure as Nesterov acceleration, but with random parameters. We provide continuized Nesterov acceleration under deterministic as well as stochastic gradients, with either additive or multiplicative noise. Finally, using our continuized framework and expressing the gossip averaging problem as the stochastic minimization of a certain energy function, we provide the first rigorous acceleration of asynchronous gossip algorithms.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">even2021continuized</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Continuized View on Nesterov Acceleration for Stochastic Gradient Descent and Randomized Gossip [&lt;b&gt;&lt;a href="https://blog.neurips.cc/2021/11/30/announcing-the-neurips-2021-award-recipients/?s=09"&gt;Outstanding paper award&lt;/a&gt;&lt;/b&gt;]}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Even, Mathieu and Berthier, Rapha{\"e}l and Bach, Francis and Flammarion, Nicolas and Gaillard, Pierre and Hendrikx, Hadrien and Massoulié, Laurent and Taylor, Adrien B.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">conference</abbr> </div> <div id="barre2020complexity" class="col-sm-8"> <div class="title">Complexity guarantees for Polyak steps with momentum</div> <div class="author"> <a href="https://mathbarre.github.io/" rel="external nofollow noopener" target="_blank">Mathieu Barré</a> ,  <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  and  Alexandre d’Aspremont </div> <div class="periodical"> <em>In Proceedings of the 33rd Conference on Learning Theory (COLT)</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2002.00915" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://proceedings.mlr.press/v125/barre20a/barre20a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/mathbarre/PerformanceEstimationPolyakSteps" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>In smooth strongly convex optimization, knowledge of the strong convexity parameter is critical for obtaining simple methods with accelerated rates. In this work, we study a class of methods, based on Polyak steps, where this knowledge is substituted by that of the optimal value, 𝑓∗. We first show slightly improved convergence bounds than previously known for the classical case of simple gradient descent with Polyak steps, we then derive an accelerated gradient method with Polyak steps and momentum, along with convergence guarantees.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">barre2020complexity</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Complexity guarantees for {P}olyak steps with momentum}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Barré, Mathieu and Taylor, Adrien B. and d’Aspremont, Alexandre}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 33rd Conference on Learning Theory (COLT)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{452--478}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">conference</abbr> </div> <div id="taylor19bach" class="col-sm-8"> <div class="title">Stochastic first-order methods: non-asymptotic and computer-aided analyses via potential functions</div> <div class="author"> <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  and  <a href="https://www.di.ens.fr/~fbach/" rel="external nofollow noopener" target="_blank">Francis Bach</a> </div> <div class="periodical"> <em>In Proceedings of the 32nd Conference on Learning Theory (COLT)</em> , 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1902.00947" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="http://proceedings.mlr.press/v99/taylor19a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/AdrienTaylor/Potential-functions-for-first-order-methods" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We provide a novel computer-assisted technique for systematically analyzing first-order methods for optimization. In contrast with previous works, the approach is particularly suited for handling sublinear convergence rates and stochastic oracles. The technique relies on semidefinite programming and potential functions. It allows simultaneously obtaining worst-case guarantees on the behavior of those algorithms, and assisting in choosing appropriate parameters for tuning their worst-case performances. The technique also benefits from comfortable tightness guarantees, meaning that unsatisfactory results can be improved only by changing the setting. We use the approach for analyzing deterministic and stochastic first-order methods under different assumptions on the nature of the stochastic noise. Among others, we treat unstructured noise with bounded variance, different noise models arising in over-parametrized expectation minimization problems, and randomized block-coordinate descent schemes.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">conference</abbr> </div> <div id="taylor2018lyapunov" class="col-sm-8"> <div class="title">Lyapunov functions for first-order methods: Tight automated convergence guarantees</div> <div class="author"> <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  <a href="https://vanscoy.github.io/" rel="external nofollow noopener" target="_blank">Bryan Van Scoy</a> ,  and  <a href="https://laurentlessard.com/" rel="external nofollow noopener" target="_blank">Laurent Lessard</a> </div> <div class="periodical"> <em>In Proceedings of the 35th International Conference on Machine Learning (ICML)</em> , 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1803.06073" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="http://proceedings.mlr.press/v80/taylor18a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/QCGroup/quad-lyap-first-order" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We present a novel way of generating Lyapunov functions for proving linear convergence rates of first-order optimization methods. Our approach provably obtains the fastest linear convergence rate that can be verified by a quadratic Lyapunov function (with given states), and only relies on solving a small-sized semidefinite program. Our approach combines the advantages of performance estimation problems (PEP, due to Drori and Teboulle (2014)) and integral quadratic constraints (IQC, due to Lessard et al. (2016)), and relies on convex interpolation (due to Taylor et al. (2017c;b)).</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">conference</abbr> </div> <div id="taylor2017performance" class="col-sm-8"> <div class="title">Performance estimation toolbox (PESTO): automated worst-case analysis of first-order optimization methods</div> <div class="author"> <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> ,  <a href="https://perso.uclouvain.be/julien.hendrickx" rel="external nofollow noopener" target="_blank">Julien M. Hendrickx</a> ,  and  <a href="https://perso.uclouvain.be/francois.glineur/" rel="external nofollow noopener" target="_blank">François Glineur</a> </div> <div class="periodical"> <em>In Proceedings of the 56th Conference on Decision and Control (CDC)</em> , 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8263832" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/AdrienTaylor/Performance-Estimation-Toolbox/blob/master/PESTO_CDC2017_FINAL.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/AdrienTaylor/Performance-Estimation-Toolbox" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We present a MATLAB toolbox that automatically computes tight worst-case performance guarantees for a broad class of first-order methods for convex optimization. The class of methods includes those performing explicit, projected, proximal, conditional and inexact (sub)gradient steps. The toolbox relies on the performance estimation (PE) framework, which recently emerged through works of Drori and Teboulle and the authors. The PE approach is a very systematic manner of obtaining non-improvable worst-case guarantees for first-order numerical optimization schemes. However, using the PE methodology requires modelling efforts from the user, along with some knowledge of semidefinite programming. The goal of this work is to ease the use of the performance estimation methodology, by providing a toolbox that implicitly does the modelling job. In short, its aim is to (i) let the user write the algorithm in a natural way, as he/she would have implemented it, and (ii) let the computer perform the modelling and worst-case analysis parts automatically.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">5 - PhDtheses</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">PhDthese</abbr> </div> <div id="taylor2017convex" class="col-sm-8"> <div class="title">Convex Interpolation and Performance Estimation of First-order Methods for Convex Optimization [<b><a href="https://uclouvain.be/en/research-institutes/icteam/news/thesis-award-2018.html" rel="external nofollow noopener" target="_blank">ICTEAM thesis award</a></b>; <b><a href="https://www.frs-fnrs.be/docs/Prix/FRS-FNRS_Liste-des-laureats-Prix_IBM_Belgium_dinformatique.pdf" rel="external nofollow noopener" target="_blank">IBM-FNRS innovation award</a></b>; <b><a href="http://www.mathopt.org/?nav=tucker" rel="external nofollow noopener" target="_blank">AW Tucker prize finalist</a></b>]</div> <div class="author"> <a href="https://adrientaylor.github.io/" rel="external nofollow noopener" target="_blank">Adrien B. Taylor</a> </div> <div class="periodical"> <em>Université catholique de Louvain</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://github.com/AdrienTaylor/Performance-Estimation-Toolbox" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The goal of this thesis is to show how to derive in a completely automated way exact and global worst-case guarantees for first-order methods in convex optimization. To this end, we formulate a generic optimization problem looking for the worst-case scenarios. The worst-case computation problems, referred to as performance estimation problems (PEPs), are intrinsically infinite-dimensional optimization problems formulated over a given class of objective functions. To render those problems tractable, we develop (smooth and non-smooth) convex interpolation framework, which provides necessary and sufficient conditions to interpolate our objective functions. With this idea, we transform PEPs into solvable finite-dimensional semidefinite programs, from which one obtains worst-case guarantees and worst-case functions, along with the corresponding explicit proofs. PEPs already proved themselves very useful as a tool for developing convergence analyses of first-order optimization methods. Among others, PEPs allow obtaining exact guarantees for gradient methods, along with their inexact, projected, proximal, conditional, decentralized and accelerated versions.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Adrien Taylor. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>